{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4895ef66",
   "metadata": {},
   "source": [
    "# CA1 – Data Cleaning and Preparation Using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc0ad82",
   "metadata": {},
   "source": [
    "## 1 – Describe and Rename Columns (10 marks)\n",
    "\n",
    "• Explore the dataset and describe what kind of data each column contains.\n",
    "\n",
    "• Some column names are abbreviations (e.g., wdspd). Rename columns to meaningful names that describe the data clearly.\n",
    "\n",
    "• You may research the dataset on Kaggle or other online sources to understand what each column represents.\n",
    "\n",
    "• The goal is to make the dataset self-explanatory and easy to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7fcb06",
   "metadata": {},
   "source": [
    "• Explore the dataset and describe what kind of data each column contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d97b51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15765/2551783462.py:3: DtypeWarning: Columns (5,6,7,8,9,10,11,12,13,14,15,16,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  weather_data = pd.read_csv('hrly_Irish_weather.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  {(4660423, 18)}\n",
      "Columns: ['county', 'station', 'latitude', 'longitude', 'date', 'rain', 'temp', 'wetb', 'dewpt', 'vappr', 'rhum', 'msl', 'wdsp', 'wddir', 'sun', 'vis', 'clht', 'clamt']\n",
      "First 5 rows:\n",
      "   county  station  latitude  longitude               date rain  temp  wetb  \\\n",
      "0  Galway  ATHENRY    53.289     -8.786  26-jun-2011 01:00  0.0  15.3  14.5   \n",
      "1  Galway  ATHENRY    53.289     -8.786  26-jun-2011 02:00  0.0  14.7  13.7   \n",
      "2  Galway  ATHENRY    53.289     -8.786  26-jun-2011 03:00  0.0  14.3  13.4   \n",
      "3  Galway  ATHENRY    53.289     -8.786  26-jun-2011 04:00  0.0  14.4  13.6   \n",
      "4  Galway  ATHENRY    53.289     -8.786  26-jun-2011 05:00  0.0  14.4  13.5   \n",
      "\n",
      "  dewpt vappr rhum     msl wdsp wddir  sun  vis clht clamt  \n",
      "0  13.9  15.8   90  1016.0    8   190  NaN  NaN  NaN   NaN  \n",
      "1  12.9  14.9   89  1015.8    7   190  NaN  NaN  NaN   NaN  \n",
      "2  12.6  14.6   89  1015.5    6   190  NaN  NaN  NaN   NaN  \n",
      "3  12.8  14.8   90  1015.3    7   180  NaN  NaN  NaN   NaN  \n",
      "4  12.7  14.7   89  1015.1    6   190  NaN  NaN  NaN   NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Loads a sample of hourly Irish weather data.\n",
    "weather_data = pd.read_csv('hrly_Irish_weather.csv')\n",
    "\n",
    "# Show the shape (rows, columns) of the loaded sample\n",
    "print(\"Shape: \" , {weather_data.shape})\n",
    "\n",
    "# List the column names so you can confirm headers and spot any naming issues\n",
    "print(\"Columns:\", weather_data.columns.tolist())\n",
    "\n",
    "# Print a small preview (first 5 rows) to inspect values and identify missing/non-sense entries\n",
    "print(\"First 5 rows:\")\n",
    "print(weather_data.iloc[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2180be",
   "metadata": {},
   "source": [
    "• Some column names are abbreviations (e.g., wdspd). Rename columns to meaningful names that describe the data clearly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7edc9e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column names:\n",
      "['county', 'station', 'latitude', 'longitude', 'date', 'rain', 'temp', 'Wet Bulb Air Temperature C', 'Dew Point Air Temperature C', 'Vapour Pressure', 'Relative Humidity', 'Mean Sea Level Pressure', 'Mean Hourly Wind Speed', 'Predominant Hourly wind Direction', 'sun', 'Visibility', 'Cloud Ceiling Height', 'Cloud Amount']\n"
     ]
    }
   ],
   "source": [
    "# Create a comprehensive mapping based on your actual columns\n",
    "# reference:\n",
    "# https://www.met.ie/cms/assets/uploads/2018/05/KeyHourly.txt\n",
    "# https://stackoverflow.com/questions/11346283/renaming-column-names-in-pandas\n",
    "\n",
    "column_mapping = {\n",
    "    'wetb': 'Wet Bulb Air Temperature C',\n",
    "    'dewpt': 'Dew Point Air Temperature C',\n",
    "    'vappr': 'Vapour Pressure',\n",
    "    'rhum': 'Relative Humidity',\n",
    "    'msl': 'Mean Sea Level Pressure',\n",
    "    'wdsp': 'Mean Hourly Wind Speed',\n",
    "    'wddir': 'Predominant Hourly wind Direction',\n",
    "    'vis': 'Visibility',\n",
    "    'clht': 'Cloud Ceiling Height',\n",
    "    'clamt': 'Cloud Amount'\n",
    "}\n",
    "\n",
    "# Rename the columns\n",
    "weather_data = weather_data.rename (columns=column_mapping)\n",
    "\n",
    "# Verify the changes\n",
    "print(\"New column names:\")\n",
    "print(weather_data.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4055886",
   "metadata": {},
   "source": [
    "## 2 – Identify Missing and Non-sense Values (10 marks)\n",
    "• Investigate the dataset to find all missing, null, and non-sense values.\n",
    "\n",
    "• Non-sense values include entries like \"?\", \"error\", \"missing\", \"NaN\", or other inconsistent symbols.\n",
    "\n",
    "• Report which columns contain such values and how many appear in each column.\n",
    "\n",
    "• Summarise your findings clearly using printed outputs and a markdown explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e9375c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "county                                     0\n",
      "station                                    0\n",
      "latitude                                   0\n",
      "longitude                                  0\n",
      "date                                       0\n",
      "rain                                       0\n",
      "temp                                       0\n",
      "Wet Bulb Air Temperature C                 0\n",
      "Dew Point Air Temperature C                0\n",
      "Vapour Pressure                            0\n",
      "Relative Humidity                          0\n",
      "Mean Sea Level Pressure                    0\n",
      "Mean Hourly Wind Speed                229032\n",
      "Predominant Hourly wind Direction     229032\n",
      "sun                                  2585167\n",
      "Visibility                           2585167\n",
      "Cloud Ceiling Height                 2585167\n",
      "Cloud Amount                         2585167\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_data = weather_data.isnull().sum()\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6940b71",
   "metadata": {},
   "source": [
    "The data below has the most missing data\n",
    "\n",
    "Mean Hourly Wind Speed                229032\n",
    "\n",
    "Predominant Hourly wind Direction     229032\n",
    "\n",
    "sun                                  2585167\n",
    "\n",
    "Visibility                           2585167\n",
    "\n",
    "Cloud Ceiling Height                 2585167\n",
    "\n",
    "Cloud Amount                         2585167"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c66609f",
   "metadata": {},
   "source": [
    "## 3 – Develop and Apply a Cleaning Strategy (15 marks)\n",
    "\n",
    "• Develop a clear and well-structured strategy to clean missing and non-sense data.\n",
    "\n",
    "• A missing value can be a blank cell, while non-sense values may include symbols or strings that do not represent real data.\n",
    "\n",
    "• Apply your cleaning strategy systematically using Pandas, and clearly justify the methods you choose (for example, why you replaced, removed, or imputed specific\n",
    "values).\n",
    "\n",
    "• Explain your cleaning steps using markdown cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c38c41fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 2585167 rows with missing values\n"
     ]
    }
   ],
   "source": [
    "# Remove any row with missing values\n",
    "weather_data_clean = weather_data.dropna()\n",
    "\n",
    "print(f\"Removed {len(weather_data) - len(weather_data_clean)} rows with missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e795cf65",
   "metadata": {},
   "source": [
    "# 4 – Detect Outliers (10 marks)\n",
    "\n",
    "• Examine the dataset for possible outliers using an appropriate statistical method.\n",
    "\n",
    "• You may use descriptive statistical tests (e.g., IQR or z-score).\n",
    "\n",
    "• Report which columns contain outliers and describe how you identified them. Explain\n",
    "it using markdown cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "666a9ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Outlier detection\n",
      "Index(['latitude', 'longitude'], dtype='object')\n",
      "latitude: 0 outliers\n",
      "longitude: 0 outliers\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\" Outlier detection\")\n",
    "# Select only numeric columns from the dataset\n",
    "numeric_cols = weather_data.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "print(numeric_cols)\n",
    "\n",
    "# Iterate through each numeric column to detect outliers\n",
    "for col in numeric_cols:\n",
    "    # Remove NaN values from the column before analysis\n",
    "    data = weather_data[col].dropna()\n",
    "    if len(data) == 0:  # Skip if column is empty after removing NaN\n",
    "        continue\n",
    "        \n",
    "    # Calculate quartiles and IQR\n",
    "    Q1 = data.quantile(0.25)  # First quartile\n",
    "    Q3 = data.quantile(0.75)  # Third quartile\n",
    "    IQR = Q3 - Q1            # Interquartile range\n",
    "    \n",
    "    # Detect outliers using 1.5 * IQR rule\n",
    "    # Values < (Q1 - 1.5*IQR) or > (Q3 + 1.5*IQR) are considered outliers\n",
    "    outliers = data[(data < Q1 - 1.5 * IQR) | (data > Q3 + 1.5 * IQR)].count()\n",
    "    print(f\"{col}: {outliers} outliers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c19707b",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "https://www.geeksforgeeks.org/python/how-to-use-pandas-filter-with-iqr/\n",
    "\n",
    "https://www.geeksforgeeks.org/data-science/detect-and-remove-the-outliers-using-python/\n",
    "\n",
    "https://www.geeksforgeeks.org/dsa/interquartile-range-iqr/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a0d0db",
   "metadata": {},
   "source": [
    "# 5 – Handle Outliers (10 marks)\n",
    "\n",
    "• Choose and apply suitable methods to handle detected outliers.\n",
    "\n",
    "• Document your reasoning and the steps you take to address them (e.g., removing,\n",
    "capping, or transforming values).\n",
    "\n",
    "• Demonstrate the effect of your outlier-handling process on the dataset. For example\n",
    "you can compare the mean value of column having outliers before and after handling\n",
    "it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d86cb90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers capped!\n"
     ]
    }
   ],
   "source": [
    "# Minimal outlier capping\n",
    "df_clean = weather_data.copy()\n",
    "\n",
    "for col in df_clean.select_dtypes(include='number'):\n",
    "    Q1, Q3 = df_clean[col].quantile([0.25, 0.75])\n",
    "    lower = Q1 - 1.5 * (Q3 - Q1)\n",
    "    upper = Q3 + 1.5 * (Q3 - Q1)\n",
    "    df_clean[col] = df_clean[col].clip(lower, upper)\n",
    "\n",
    "print(\"Outliers capped!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3debc3b8",
   "metadata": {},
   "source": [
    "# 6 – Check and Sort by Date (10 marks)\n",
    "\n",
    "• Examine whether the dataset is properly sorted by its date or time column.\n",
    "\n",
    "• If it is not sorted, reorder it chronologically.\n",
    "\n",
    "• Confirm that sorting was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c893623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting date column to datetime format.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15765/2467628822.py:7: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_clean[date_col] = pd.to_datetime(df_clean[date_col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date conversion completed.\n",
      "Checking date sorting.\n",
      "Dataset is not sorted by date. Sorting chronologically.\n",
      "Sorting completed successfully!\n",
      "First date after sorting: 1990-01-01 00:00:00\n",
      "Last date after sorting: 2020-06-01 00:00:00\n",
      "Date range: 1990-01-01 to 2020-06-01\n"
     ]
    }
   ],
   "source": [
    "# First, ensure the date column is in proper datetime format\n",
    "date_col = 'date'\n",
    "\n",
    "# Convert to datetime if not already done (using pandas as shown in notes)\n",
    "pd.api.types.is_datetime64_any_dtype(df_clean[date_col])\n",
    "print(\"Converting date column to datetime format.\")\n",
    "df_clean[date_col] = pd.to_datetime(df_clean[date_col])\n",
    "print(\"Date conversion completed.\")\n",
    "\n",
    "print(\"Checking date sorting.\")\n",
    "\n",
    "# Check if the dates are monotonically increasing (properly sorted)\n",
    "if not df_clean[date_col].is_monotonic_increasing:\n",
    "    print(\"Dataset is not sorted by date. Sorting chronologically.\")\n",
    "    \n",
    "    # Sort by date column\n",
    "    df_clean = df_clean.sort_values(date_col)\n",
    "    \n",
    "    # Reset index after sorting to maintain proper order\n",
    "    df_clean = df_clean.reset_index(drop=True)\n",
    "    \n",
    "    print(\"Sorting completed successfully!\")\n",
    "    \n",
    "    # Display some basic info about the sorted dates\n",
    "    print(f\"First date after sorting: {df_clean[date_col].iloc[0]}\")\n",
    "    print(f\"Last date after sorting: {df_clean[date_col].iloc[-1]}\")\n",
    "else:\n",
    "    print(\"Dataset is already properly sorted by date.\")\n",
    "\n",
    "# Display the date range (using datetime formatting from notes)\n",
    "print(f\"Date range: {df_clean[date_col].min().strftime('%Y-%m-%d')} to {df_clean[date_col].max().strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd57c83b",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "lecture7_dates.pdf\n",
    "\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html#pandas.DataFrame.sort_values\n",
    "\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.Series.is_monotonic_increasing.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e8696b",
   "metadata": {},
   "source": [
    "# 7– Date-based Slicing (10 marks)\n",
    "\n",
    "• Use Pandas slicing to extract and analyse data based on specific date ranges. Select\n",
    "data for the month of your birth in a year of your choice and select columns “rain” and\n",
    "“temp”\n",
    "\n",
    "• Calculate the average rainfall and average temperature for that month.\n",
    "\n",
    "• Present your results clearly with code and markdown explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ded5334",
   "metadata": {},
   "source": [
    "First I Converted date, temp, rainfall to numbers. Checking where the data start date till the end from ( 1990-01-01 00:00:00 to 2020-06-01 00:00:00 )\n",
    "Checked the date with my birth year and month for the average rainfall in mm and temperature C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33287772",
   "metadata": {},
   "source": [
    "# Task 8 – Location-based Slicing (15 marks)\n",
    "\n",
    "• Select your favourite Irish county or weather station and perform an analysis of\n",
    "weather patterns there.\n",
    "\n",
    "• Identify which months or seasons are best for visiting based on temperature and\n",
    "rainfall data. For this task you need to compare your calculated statistics with the\n",
    "ideal conditions and select days of the months in each year for those conditions. For\n",
    "example, good days to visit Dublin are the ones when temperature above 20 °C and\n",
    "low chances of rainfall. Create similar conditional statements using suitable Pandas\n",
    "functions and display suitable days range in each year.\n",
    "\n",
    "• Summarise your insights with calculations and a short explanation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a12a539",
   "metadata": {},
   "source": [
    "# Task 9 – Remove Empty or Irrelevant Columns (5 marks)\n",
    "\n",
    "• Identify any columns that are empty or do not have any useful information.\n",
    "\n",
    "• Remove such columns and display the shape of the final cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b7dbfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (4660423, 18)\n",
      "Final cleaned dataset shape: (4660423, 18)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the data with proper missing value handling\n",
    "weather_data = pd.read_csv('hrly_Irish_weather.csv', na_values=[\"N/A\", \"NaN\", \"NA\", \" \"])\n",
    "\n",
    "print(\"Original dataset shape:\", weather_data.shape)\n",
    "\n",
    "# Remove empty columns (all values are NaN)\n",
    "weather_data_cleaned = weather_data.dropna(axis='columns', how='all')\n",
    "\n",
    "print(\"Final cleaned dataset shape:\", weather_data_cleaned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d12001",
   "metadata": {},
   "source": [
    "# Task 10 – Save and Submit (5 marks)\n",
    "• Save your cleaned dataset as: cleaned_hrly_Irish_weather.csv\n",
    "• Ensure that all data cleaning and processing steps are completed within the same\n",
    "notebook.\n",
    "\n",
    "• Submit both your cleaned dataset and your Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac42bd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved successfully as: cleaned_hrly_Irish_weather.csv\n",
      "File contains 4660423 rows and 18 columns\n"
     ]
    }
   ],
   "source": [
    "# Save the cleaned dataset\n",
    "weather_data.to_csv('cleaned_hrly_Irish_weather.csv', index=False)\n",
    "\n",
    "print(\"Dataset saved successfully as: cleaned_hrly_Irish_weather.csv\")\n",
    "print(f\"File contains {len(weather_data)} rows and {len(weather_data.columns)} columns\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
